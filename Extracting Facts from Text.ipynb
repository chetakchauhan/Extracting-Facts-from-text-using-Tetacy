{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "\n",
    "import spacy\n",
    "import textacy.extract\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Python library textacy implements several common data extraction algorithms on top of spaCy, \n",
    "# including semi-structured statement extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text that we want to examine\n",
    "text = Path(\"london.txt\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the document with spaCy\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract semi-structured statements\n",
    "statements = textacy.extract.semistructured_statements(doc, \"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the things I know about London:\n",
      "\n",
      "- the capital and most populous city of England and the United Kingdom.\n",
      "\n",
      "- a major settlement for two millennia\n",
      "- the world's most populous city from around 1831 to 1925\n",
      "- beyond all comparison the largest\n",
      "town in England\n",
      "- still very compact\n",
      "- the world's largest city from about 1831 to 1925\n",
      "- the seat of the\n",
      "Government of the United Kingdom\n",
      "- vulnerable to flooding.\n",
      "\n",
      "- \"one of the World's\n",
      "Greenest Cities\" with more than 40 percent green space or open water\n",
      "- the most\n",
      "populous city and metropolitan area of the European Union and the second most\n",
      "populous in Europe\n",
      "- the 19th largest city and the 18th largest\n",
      "metropolitan region in the world\n",
      "- Christian, and has a large number of churches, particularly\n",
      "in the City of London\n",
      "- also home to sizeable Muslim, Hindu, Sikh, and Jewish\n",
      "communities\n",
      "- also home to 42\n",
      "Hindu temples\n",
      "- one of the pre-eminent financial centres of the world as the most\n",
      "important location for international finance\n",
      "- the world top city\n",
      "destination as ranked by TripAdvisor users\n",
      "- a major\n",
      "international air transport hub with the busiest city airspace in the world.\n",
      "\n",
      "- the centre of the National Rail network, with 70 percent of rail journeys\n",
      "starting or ending in London\n",
      "- home to five major medical schools\n",
      "- home to designers Vivienne Westwood, Galliano, Stella\n",
      "McCartney, Manolo Blahnik, and Jimmy Choo, among others\n",
      "- the setting for many works of\n",
      "literature\n",
      "- also a centre for urban music\n",
      "- the \"greenest city\" in Europe with 35,000 acres\n",
      "of public parks, woodlands and gardens\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Here are the things I know about London:\\n\")\n",
    "\n",
    "for statement in statements:\n",
    "    subject, verb, fact = statement\n",
    "    print(f\"- {fact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the algorithms included in textacy is called noun chunk extraction. It looks for chunks of words that \n",
    "# seem to belong together and seem to refer to a single idea. These kinds of words are often the kinds of \n",
    "# keywords that a user will type into a search box. We can use them to populate our autocomplete system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract semi-structured statements\n",
    "noun_chunks = textacy.extract.noun_chunks(doc, min_freq=3)\n",
    "\n",
    "# The min_freq=3 parameter tells textacy to ignore any noun chunks that don’t appear at least three times in the \n",
    "# document. We are collecting common terms, so we don’t need every possible noun in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert noun chunks to lowercase strings\n",
    "noun_chunks = map(str, noun_chunks)\n",
    "noun_chunks = map(str.lower, noun_chunks)\n",
    "\n",
    "# The list of noun chunks we get back will be spaCy tokens and the words will be a mix of uppercase and lowercase.\n",
    "# So we need to convert them to lowercase strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "city centre\n",
      "eight royal parks\n",
      "european union\n",
      "inner london\n",
      "new york city\n",
      "westminster abbey\n",
      "\n",
      "london school\n",
      "\n",
      "national gallery\n",
      "large number\n",
      "south london\n",
      "\n",
      "eight royal parks\n",
      "london's population\n",
      "london eye\n",
      "london school\n",
      "national statistics\n",
      "outer london\n",
      "west london\n",
      "\n",
      "major centre\n",
      "regent's park\n",
      "second world war\n",
      "greater london's population\n",
      "royal opera house\n",
      "london underground\n",
      "greater london authority\n",
      "central london\n",
      "major centre\n",
      "greater london\n",
      "2011 census\n",
      "trafalgar square\n",
      "\n",
      "population density\n",
      "other city\n",
      "\n",
      "other city\n",
      "river thames\n",
      "great fire\n",
      "canary wharf\n",
      "royal albert hall\n",
      "city centre\n",
      "hampstead heath\n",
      "\n",
      "london underground\n",
      "national gallery\n",
      "tate modern\n",
      "office space\n",
      "united kingdom\n",
      "british museum\n",
      "east end\n",
      "population density\n",
      "epping forest\n"
     ]
    }
   ],
   "source": [
    "# Print out any nouns that are at least 2 words long\n",
    "for noun_chunk in set(noun_chunks):\n",
    "    if len(noun_chunk.split(\" \")) > 1:\n",
    "        print(noun_chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
